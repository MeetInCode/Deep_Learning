{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Bidirectional LSTMs (BiLSTMs) ðŸ§ \n",
        "\n",
        "This notebook explains the concept of Bidirectional Long Short-Term Memory (BiLSTM) networks and provides a practical implementation using TensorFlow and Keras.\n",
        "\n",
        "## What is an LSTM?\n",
        "\n",
        "A standard **Long Short-Term Memory (LSTM)** network is a type of Recurrent Neural Network (RNN) that is excellent at learning from sequential data, like text or time series. It processes data in a sequence, step-by-step, carrying information from past steps to future ones.\n",
        "\n",
        "**Limitation:** A standard LSTM only looks at the past. When reading a sentence, it processes words from left to right. At any given word, it only knows about the words that came *before* it.\n",
        "\n",
        "## Why do we need Bidirectionality?\n",
        "\n",
        "In many tasks, especially in Natural Language Processing (NLP), context from both the past **and** the future is crucial for understanding.\n",
        "\n",
        "Consider the sentence: \"The **bank** of the river was muddy.\"\n",
        "\n",
        "To understand that \"bank\" refers to a riverside and not a financial institution, you need to see the word \"river\" which comes *after* it. A standard LSTM would struggle here.\n",
        "\n",
        "This is where **Bidirectional LSTMs** come in!\n",
        "\n",
        "## How does a BiLSTM work?\n",
        "\n",
        "A BiLSTM is simple in concept: it's two LSTMs working together.\n",
        "\n",
        "1.  **Forward LSTM:** One LSTM processes the sequence from left-to-right (from beginning to end).\n",
        "2.  **Backward LSTM:** A second, separate LSTM processes the sequence from right-to-left (from end to beginning).\n",
        "\n",
        "At each time step (i.e., for each word), the outputs from both the forward and backward LSTMs are concatenated. This combined output gives the model a rich representation of each word, informed by both its preceding (past) and succeeding (future) context.\n",
        "\n",
        "![BiLSTM Architecture](https://i.imgur.com/h532nL4.png)\n"
      ],
      "metadata": {
        "id": "J1Gg0hB4xR0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1. Setup and Imports\n",
        "\n",
        "Let's start by importing the necessary libraries from TensorFlow and Keras."
      ],
      "metadata": {
        "id": "3l3J7mKcxR0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "r7c1H6naxR0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. Data Loading and Preprocessing\n",
        "\n",
        "We will use the **IMDB movie review dataset**, a classic dataset for binary sentiment classification (positive/negative).\n",
        "\n",
        "We will perform two key preprocessing steps:\n",
        "1.  **Vocabulary Limit:** We'll only consider the top 10,000 most frequent words to keep our model's vocabulary manageable.\n",
        "2.  **Padding:** Neural networks require inputs to have a consistent length. We will pad or truncate all movie reviews to be exactly 200 words long."
      ],
      "metadata": {
        "id": "u-D68QeRxR0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parameters ---\n",
        "max_features = 10000  # Number of words to consider as features (vocabulary size)\n",
        "maxlen = 200        # Max length of sequences (reviews)\n",
        "\n",
        "# --- Load the data ---\n",
        "print(\"Loading data...\")\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "# --- Pad sequences ---\n",
        "# This ensures all sequences in a list have the same length.\n",
        "print(\"\\nPad sequences (samples x time)\")\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "metadata": {
        "id": "UeJ7x24CxR0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a82f34-31ea-4217-efd9-b9c1044458f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464779/17464779 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 200)\n",
            "x_test shape: (25000, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Building the BiLSTM Model\n",
        "\n",
        "Now, let's define our model architecture. It will have three main layers:\n",
        "\n",
        "1.  `Embedding` Layer: This layer takes the integer-encoded vocabulary and maps each word index to a dense vector of a specified size (in our case, 128). This helps the model learn relationships between words.\n",
        "\n",
        "2.  `Bidirectional(LSTM(...))` Layer: This is the core of our model. We wrap a standard `LSTM` layer inside a `Bidirectional` wrapper. Keras handles the creation of the forward and backward LSTMs automatically. The output of this layer will be the concatenated features from both directions.\n",
        "\n",
        "3.  `Dense` Layer: A standard fully connected neural network layer with a `sigmoid` activation function to output a probability between 0 (negative sentiment) and 1 (positive sentiment)."
      ],
      "metadata": {
        "id": "zY6l1H3nxR0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Definition ---\n",
        "model = Sequential()\n",
        "\n",
        "# 1. Embedding Layer\n",
        "# It turns positive integers (indexes) into dense vectors of fixed size.\n",
        "model.add(Embedding(max_features, 128))\n",
        "\n",
        "# 2. Bidirectional LSTM Layer\n",
        "# We wrap our LSTM layer in a Bidirectional wrapper.\n",
        "# The LSTM has 64 units (a hyperparameter you can tune).\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "\n",
        "# 3. Output Layer\n",
        "# A Dense layer with 1 neuron and sigmoid activation for binary classification.\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# --- Compile the model ---\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# --- Print model summary ---\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Yd3tC4DwxR0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c897f9-2c78-438a-e578-1b6c86729221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 128)         1280000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              98816     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,378,945\n",
            "Trainable params: 1,378,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the output shape of the `bidirectional` layer is `(None, 128)`. This is because the forward LSTM outputs 64 features and the backward LSTM outputs 64 features. By default, the `Bidirectional` wrapper **concatenates** them, so $64 + 64 = 128$."
      ],
      "metadata": {
        "id": "7QkS_4t9xR0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Training the Model\n",
        "\n",
        "Let's train our model. We'll use a `batch_size` of 32 and train for 5 epochs. We'll also use 20% of our training data for validation during training to monitor performance on unseen data."
      ],
      "metadata": {
        "id": "s7-tWwLgxR0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training model...\")\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=5,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "id": "vM1p8V3dxR0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6f991f-1358-47ac-c774-2977f6b8b091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Epoch 1/5\n",
            "625/625 [==============================] - 58s 85ms/step - loss: 0.4357 - accuracy: 0.7936 - val_loss: 0.3204 - val_accuracy: 0.8650\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.2372 - accuracy: 0.9079 - val_loss: 0.3421 - val_accuracy: 0.8642\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 21s 33ms/step - loss: 0.1472 - accuracy: 0.9463 - val_loss: 0.3754 - val_accuracy: 0.8638\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.0967 - accuracy: 0.9663 - val_loss: 0.4485 - val_accuracy: 0.8528\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.0594 - accuracy: 0.9806 - val_loss: 0.5365 - val_accuracy: 0.8576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 5. Evaluating the Model\n",
        "\n",
        "Now that the model is trained, let's evaluate its performance on the hold-out test set."
      ],
      "metadata": {
        "id": "e_zYt0qDxR0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(f\"Test score: {score:.4f}\")\n",
        "print(f\"Test accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "59L9tUaUxR0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b08e734-6447-495c-9c98-4235e982ec45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 6s 8ms/step - loss: 0.5750 - accuracy: 0.8465\n",
            "Test score: 0.5750\n",
            "Test accuracy: 0.8465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 6. Making Predictions on New Data\n",
        "\n",
        "Let's write a simple function to see our model in action. This function will take a raw text sentence, preprocess it in the same way as our training data, and predict the sentiment."
      ],
      "metadata": {
        "id": "u4W7rM0JxR0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the word index from the IMDB dataset\n",
        "word_index = imdb.get_word_index()\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # Unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    # Preprocess the text\n",
        "    # 1. Tokenize: Convert text to a sequence of integers\n",
        "    words = text.lower().split()\n",
        "    sequence = [word_index.get(word, 2) for word in words] # Use 2 for unknown words\n",
        "\n",
        "    # 2. Pad the sequence\n",
        "    padded_sequence = pad_sequences([sequence], maxlen=maxlen)\n",
        "\n",
        "    # 3. Predict\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    probability = prediction[0][0]\n",
        "\n",
        "    print(f\"\\nReview: '{text}'\")\n",
        "    print(f\"Positive Sentiment Probability: {probability:.4f}\")\n",
        "\n",
        "    if probability > 0.5:\n",
        "        print(\"Result: Positive sentiment ðŸ˜Š\")\n",
        "    else:\n",
        "        print(\"Result: Negative sentiment ðŸ˜ž\")\n",
        "\n",
        "# --- Test with some reviews ---\n",
        "predict_sentiment(\"This was an absolutely fantastic film with brilliant acting and a great plot\")\n",
        "predict_sentiment(\"The movie was a complete waste of time boring and predictable\")"
      ],
      "metadata": {
        "id": "41tJzJtMxR0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac341f2-1a48-43e8-8b96-7d04587c6ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n",
            "\n",
            "1/1 [==============================] - 0s 499ms/step\n",
            "\n",
            "Review: 'This was an absolutely fantastic film with brilliant acting and a great plot'\n",
            "Positive Sentiment Probability: 0.9818\n",
            "Result: Positive sentiment ðŸ˜Š\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "Review: 'The movie was a complete waste of time boring and predictable'\n",
            "Positive Sentiment Probability: 0.0076\n",
            "Result: Negative sentiment ðŸ˜ž\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, we learned that:\n",
        "\n",
        "-   Standard LSTMs process sequences in one direction (past to present).\n",
        "-   This limits their ability to capture context, as the meaning of a word often depends on words that come after it.\n",
        "-   **Bidirectional LSTMs** solve this by using two LSTMsâ€”one processing the sequence forward and one backward.\n",
        "-   The outputs are concatenated, providing a much richer contextual representation for each element in the sequence.\n",
        "-   Implementing a BiLSTM in Keras is straightforward using the `Bidirectional` layer wrapper.\n",
        "\n",
        "For many NLP tasks like sentiment analysis, named entity recognition, and machine translation, BiLSTMs often outperform their unidirectional counterparts."
      ],
      "metadata": {
        "id": "4x_u4r9QxR0o"
      }
    }
  ]
}